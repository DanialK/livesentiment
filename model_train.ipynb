{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danial/DL/livesentiment/ENV/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import CSVLogger\n",
    "import operator\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import nltk as nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import random\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "ps = PorterStemmer()\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, History\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = './'\n",
    "GLOVE_DIR = './embeddings'\n",
    "MAX_SEQUENCE_LENGTH = 256\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the Glove embeddings from here https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, './glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minority_balance_dataframe_by_multiple_categorical_variables(df, categorical_columns=None, downsample_by=0.1):\n",
    "    \"\"\"\n",
    "    :param df: pandas.DataFrame\n",
    "    :param categorical_columns: iterable of categorical columns names contained in {df}\n",
    "    :return: balanced pandas.DataFrame\n",
    "    \"\"\"\n",
    "    if categorical_columns is None or not all([c in df.columns for c in categorical_columns]):\n",
    "        raise ValueError('Please provide one or more columns containing categorical variables')\n",
    "\n",
    "    minority_class_combination_count = df.groupby(categorical_columns).apply(lambda x: x.shape[0]).min()\n",
    "    \n",
    "    minority_class_combination_count = int(minority_class_combination_count * downsample_by)\n",
    "    \n",
    "    df = df.groupby(categorical_columns).apply(\n",
    "        lambda x: x.sample(minority_class_combination_count)\n",
    "    ).drop(categorical_columns, axis=1).reset_index().set_index('level_1')\n",
    "\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_features_for_layer(X, trained_model, layer_number, batches=256):\n",
    "    \"\"\"\n",
    "    :param X: Batch with dimensions according to the models first layer input-shape\n",
    "    :param trained_model: Model to extract data from\n",
    "    :param layer_number: Index of the layer we want to extract features from.\n",
    "    :param batches: If set it will call the function in batches to save (gpu)memory\n",
    "    :return: \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    get_features = K.function([trained_model.layers[0].input, K.learning_phase()],\n",
    "                              [trained_model.layers[layer_number].output])\n",
    "    \n",
    "    if batches:\n",
    "        g = array_batch_yield(X, batches)\n",
    "        features = []\n",
    "        for batch in g:\n",
    "            feature_batch = get_features([batch, 0])\n",
    "            features.append(feature_batch)\n",
    "            \n",
    "        features = np.concatenate(features, axis=1)[0]\n",
    "        \n",
    "    else:\n",
    "        features = get_features([X, 0])\n",
    "\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def array_batch_yield(X, group_size):\n",
    "    for i in xrange(0, len(X), group_size):\n",
    "        yield X[i:i+group_size]\n",
    "        \n",
    "langdetect_count = 0\n",
    "def safe_detect(s):\n",
    "    try:\n",
    "        global langdetect_count\n",
    "        count+=1    \n",
    "        if langdetect_count % 10000 == 0:\n",
    "            print(\"Detected languages for  {} reviews\".format(count))       \n",
    "        return langdetect.detect(s)\n",
    "    except:\n",
    "        return 'unknown'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the Yelp Dataset from here https://www.yelp.com/dataset and read the review.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews =  pd.read_json('./data/review.json', lines=True, encoding='utf-8')\n",
    "df_reviews['len'] = df_reviews.text.str.len()\n",
    "df_reviews = df_reviews[df_reviews['len'].between(10, 4000)]\n",
    "\n",
    "# balancing dataset\n",
    "df_rev_balanced = minority_balance_dataframe_by_multiple_categorical_variables(\n",
    "    df_reviews, \n",
    "    categorical_columns=['stars'], \n",
    "    downsample_by=0.1\n",
    ")\n",
    "\n",
    "df_rev_balanced.to_csv('balanced_reviews.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(df_rev_balanced.text.tolist())\n",
    "joblib.dump(tokenizer, 'tokenizer.pickle')\n",
    "\n",
    "WORD_INDEX_SORTED = sorted(tokenizer.word_index.items(), key=operator.itemgetter(1))\n",
    "\n",
    "seqs = tokenizer.texts_to_sequences(df_rev_balanced.text.values)\n",
    "X = pad_sequences(seqs, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "Y = df_rev_balanced.stars.values.astype(int)\n",
    "Y_cat = [1 if y > 3 else 0 for y in Y]\n",
    "assert X.shape[0] == Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y_cat, test_size=0.2, random_state=9)\n",
    "with pd.HDFStore('x_y_test_train.h5') as h:\n",
    "    h['X_train'] = pd.DataFrame(X_train)\n",
    "    h['X_test'] = pd.DataFrame(X_test)\n",
    "    h['y_train'] = pd.DataFrame(y_train)\n",
    "    h['y_test'] = pd.DataFrame(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pre-prossed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev_balanced = pd.read_csv('balanced_reviews.csv')\n",
    "tokenizer = joblib.load('tokenizer.pickle')\n",
    "with pd.HDFStore('x_y_test_train.h5') as h:\n",
    "    X_train = h['X_train'].values\n",
    "    X_test = h['X_test'].values\n",
    "    y_train = h['y_train'].values\n",
    "    y_test = h['y_test'].values\n",
    "WORD_INDEX_SORTED = sorted(tokenizer.word_index.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(WORD_INDEX_SORTED))\n",
    "embedding_matrix = np.zeros((MAX_NUM_WORDS, EMBEDDING_DIM))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = LSTM(\n",
    "    64,\n",
    "    kernel_initializer='glorot_normal',\n",
    "    recurrent_initializer='glorot_normal'\n",
    ")(embedded_sequences)\n",
    "preds = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 256, 100)          2000000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 2,042,305\n",
      "Trainable params: 42,305\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 174432 samples, validate on 43608 samples\n",
      "Epoch 1/20\n",
      "174432/174432 [==============================] - 182s 1ms/step - loss: 0.4685 - acc: 0.7752 - val_loss: 0.3923 - val_acc: 0.8246\n",
      "Epoch 2/20\n",
      "174432/174432 [==============================] - 182s 1ms/step - loss: 0.3659 - acc: 0.8352 - val_loss: 0.3552 - val_acc: 0.8418\n",
      "Epoch 3/20\n",
      "174432/174432 [==============================] - 182s 1ms/step - loss: 0.3283 - acc: 0.8542 - val_loss: 0.3248 - val_acc: 0.8540\n",
      "Epoch 4/20\n",
      "174432/174432 [==============================] - 183s 1ms/step - loss: 0.3101 - acc: 0.8630 - val_loss: 0.3144 - val_acc: 0.8603\n",
      "Epoch 5/20\n",
      "174432/174432 [==============================] - 182s 1ms/step - loss: 0.2985 - acc: 0.8685 - val_loss: 0.3032 - val_acc: 0.8651\n",
      "Epoch 6/20\n",
      "174432/174432 [==============================] - 183s 1ms/step - loss: 0.2879 - acc: 0.8743 - val_loss: 0.2962 - val_acc: 0.8680\n",
      "Epoch 7/20\n",
      "174432/174432 [==============================] - 183s 1ms/step - loss: 0.2782 - acc: 0.8790 - val_loss: 0.2888 - val_acc: 0.8725\n",
      "Epoch 8/20\n",
      "174432/174432 [==============================] - 182s 1ms/step - loss: 0.2713 - acc: 0.8827 - val_loss: 0.2844 - val_acc: 0.8749\n",
      "Epoch 9/20\n",
      "174432/174432 [==============================] - 183s 1ms/step - loss: 0.2647 - acc: 0.8860 - val_loss: 0.2814 - val_acc: 0.8762\n",
      "Epoch 10/20\n",
      "174432/174432 [==============================] - 185s 1ms/step - loss: 0.2604 - acc: 0.8876 - val_loss: 0.2787 - val_acc: 0.8792\n",
      "Epoch 11/20\n",
      "174432/174432 [==============================] - 186s 1ms/step - loss: 0.2536 - acc: 0.8909 - val_loss: 0.2750 - val_acc: 0.8789\n",
      "Epoch 12/20\n",
      "174432/174432 [==============================] - 186s 1ms/step - loss: 0.2490 - acc: 0.8930 - val_loss: 0.2799 - val_acc: 0.8775\n",
      "Epoch 13/20\n",
      "174432/174432 [==============================] - 186s 1ms/step - loss: 0.2447 - acc: 0.8952 - val_loss: 0.2831 - val_acc: 0.8791\n",
      "Epoch 14/20\n",
      "174432/174432 [==============================] - 188s 1ms/step - loss: 0.2404 - acc: 0.8969 - val_loss: 0.2741 - val_acc: 0.8812\n",
      "Epoch 15/20\n",
      "174432/174432 [==============================] - 187s 1ms/step - loss: 0.2375 - acc: 0.8986 - val_loss: 0.2867 - val_acc: 0.8758\n",
      "Epoch 16/20\n",
      "174432/174432 [==============================] - 186s 1ms/step - loss: 0.2327 - acc: 0.9008 - val_loss: 0.2723 - val_acc: 0.8817\n",
      "Epoch 17/20\n",
      "174432/174432 [==============================] - 188s 1ms/step - loss: 0.2283 - acc: 0.9034 - val_loss: 0.2733 - val_acc: 0.8813\n",
      "Epoch 18/20\n",
      "174432/174432 [==============================] - 185s 1ms/step - loss: 0.2249 - acc: 0.9051 - val_loss: 0.2695 - val_acc: 0.8833\n",
      "Epoch 19/20\n",
      "174432/174432 [==============================] - 184s 1ms/step - loss: 0.2208 - acc: 0.9067 - val_loss: 0.2785 - val_acc: 0.8814\n",
      "Epoch 20/20\n",
      "174432/174432 [==============================] - 183s 1ms/step - loss: 0.2173 - acc: 0.9092 - val_loss: 0.2799 - val_acc: 0.8777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f23705cefd0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          batch_size=512,\n",
    "          epochs=20,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to YAML\n",
    "model_yaml = model.to_yaml()\n",
    "with open(\"./output/model.yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"./output/model_weights.h5\")\n",
    "model.save(\"./output/model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_yaml\n",
    "# load YAML and create model\n",
    "yaml_file = open('./output/model.yaml', 'r')\n",
    "loaded_model_yaml = yaml_file.read()\n",
    "yaml_file.close()\n",
    "model = model_from_yaml(loaded_model_yaml)\n",
    "# load weights into new model\n",
    "model.load_weights(\"./output/model_weights.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras To CoreML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coremltools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : input_1, <keras.engine.topology.InputLayer object at 0x7f2301760080>\n",
      "1 : embedding_1, <keras.layers.embeddings.Embedding object at 0x7f23017601d0>\n",
      "2 : lstm_1, <keras.layers.recurrent.LSTM object at 0x7f2301760390>\n",
      "3 : dense_1, <keras.layers.core.Dense object at 0x7f23017602b0>\n",
      "4 : dense_1__activation__, <keras.layers.core.Activation object at 0x7f2214d572b0>\n"
     ]
    }
   ],
   "source": [
    "# Convert a caffe model to a classifier in Core ML\n",
    "coreml_model = coremltools.converters.keras.convert(\n",
    "  model,\n",
    "  input_names = 'input',\n",
    "  output_names = 'output',\n",
    "  class_labels = [0, 1]\n",
    ")\n",
    "\n",
    "coreml_model.author = 'Danial Khosravi'\n",
    "coreml_model.license = 'MIT'\n",
    "coreml_model.short_description = 'Predicts the sentiment of a tokenized string'\n",
    "coreml_model.input_description['input'] = 'A String mapped according to the pre-deifned mapping'\n",
    "coreml_model.output_description['output'] = 'Whether the sentence was positive or negative'\n",
    "\n",
    "coreml_model.save('./output/model.mlmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras to TensorflowJS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/danial/DL/livesentiment/ENV/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/danial/DL/livesentiment/ENV/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import tensorflowjs as tfjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some reason models converted with tfjs.converters.save_keras_model are currenlty giving an error on the browser\n",
    "# so we're using the bash command tensorflowjs_converter \n",
    "# tfjs.converters.save_keras_model(model, './output/sentiment_js_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./output/sentiment_js_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/danial/DL/livesentiment/ENV/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:From /home/danial/DL/livesentiment/ENV/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "!tensorflowjs_converter --input_format keras ./output/model.h5 ./output/sentiment_js_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras to Tensorflow (Android)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.python.tools import optimize_for_inference_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'SentimentModel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model(saver, model, input_node_names, output_node_name):\n",
    "    tf.train.write_graph(K.get_session().graph_def, 'output', \\\n",
    "        MODEL_NAME + '_graph.pbtxt')\n",
    "\n",
    "    saver.save(K.get_session(), 'output/' + MODEL_NAME + '.chkp')\n",
    "\n",
    "    freeze_graph.freeze_graph('output/' + MODEL_NAME + '_graph.pbtxt', None, \\\n",
    "        False, 'output/' + MODEL_NAME + '.chkp', output_node_name, \\\n",
    "        \"save/restore_all\", \"save/Const:0\", \\\n",
    "        'output/frozen_' + MODEL_NAME + '.pb', True, \"\")\n",
    "\n",
    "    input_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.Open('output/frozen_' + MODEL_NAME + '.pb', \"rb\") as f:\n",
    "        input_graph_def.ParseFromString(f.read())\n",
    "\n",
    "    output_graph_def = optimize_for_inference_lib.optimize_for_inference(\n",
    "            input_graph_def, input_node_names, [output_node_name],\n",
    "            tf.float32.as_datatype_enum)\n",
    "\n",
    "    with tf.gfile.FastGFile('output/opt_' + MODEL_NAME + '.pb', \"wb\") as f:\n",
    "        f.write(output_graph_def.SerializeToString())\n",
    "\n",
    "    print(\"graph saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 256, 100)          2000000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 2,042,305\n",
      "Trainable params: 42,305\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from output/SentimentModel.chkp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from output/SentimentModel.chkp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 6 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 6 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 6 variables to const ops.\n",
      "graph saved!\n"
     ]
    }
   ],
   "source": [
    "export_model(tf.train.Saver(), model, [\"input_1\"], \"dense_1/Sigmoid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Index to SQLight (Mobile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import sys\n",
    "import re\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_file = './output/sentiment_db.sqlite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./output/sentiment_db.sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to the database file\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "c = conn.cursor()\n",
    "\n",
    "# Creating a new SQLite table with 1 column\n",
    "c.execute('CREATE TABLE word_index (key STRING, value INTEGER)')\n",
    "\n",
    "# Committing changes and closing the connection to the database file\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(sqlite_file)\n",
    "c = conn.cursor()\n",
    "\n",
    "count = 0\n",
    "for key, value in word_index.items():\n",
    "    if (key):\n",
    "        c.execute(\"INSERT INTO word_index (key, value) VALUES (\\\"{x}\\\", {y})\".\\\n",
    "            format(x=key, y=int(value)))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Index to JSON (Web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./output/word_index.json', 'w') as fp:\n",
    "    json.dump(tokenizer.word_index, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
